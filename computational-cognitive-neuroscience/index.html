<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=robots content="noodp">
<title class=pjax-title>Computational Cognitive Neuroscience - Something to Live</title><meta name=Description content="Tech Notes of Wen-Wei Tseng"><meta property="og:title" content="Computational Cognitive Neuroscience">
<meta property="og:description" content="Course Notes of Computational Cognitive Neuroscience by Prof. 鄭士康.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://sosiristseng.github.io/computational-cognitive-neuroscience/"><meta property="og:image" content="https://sosiristseng.github.io/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-06-18T17:29:03+08:00">
<meta property="article:modified_time" content="2021-06-18T17:44:45+08:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://sosiristseng.github.io/">
<meta name=twitter:title content="Computational Cognitive Neuroscience">
<meta name=twitter:description content="Course Notes of Computational Cognitive Neuroscience by Prof. 鄭士康.">
<meta name=application-name content="Los los los">
<meta name=apple-mobile-web-app-title content="Los los los">
<meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel=icon href=parrot.svg><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://sosiristseng.github.io/computational-cognitive-neuroscience/><link rel=prev href=https://sosiristseng.github.io/academic-writing-week-4/><link rel=next href=https://sosiristseng.github.io/applied-electricity/><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=preload as=style onload="this.onload=null,this.rel='stylesheet'" href=/lib/fontawesome-free/all.min.css>
<noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload as=style onload="this.onload=null,this.rel='stylesheet'" href=/lib/animate/animate.min.css>
<noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Computational Cognitive Neuroscience","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/sosiristseng.github.io\/computational-cognitive-neuroscience\/"},"genre":"posts","wordcount":4121,"url":"https:\/\/sosiristseng.github.io\/computational-cognitive-neuroscience\/","datePublished":"2021-06-18T17:29:03+08:00","dateModified":"2021-06-18T17:44:45+08:00","license":"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher":{"@type":"Organization","name":"xxxx"},"author":{"@type":"Person","name":"Wen-Wei Tseng"},"description":""}</script></head>
<body header-desktop=fixed header-mobile=auto><script type=text/javascript>function setTheme(a){document.body.setAttribute('theme',a)}function saveTheme(a){window.localStorage&&localStorage.setItem('theme',a)}function getMeta(b){const a=document.getElementsByTagName('meta');for(let c=0;c<a.length;c++)if(a[c].getAttribute('name')===b)return a[c];return''}if(window.localStorage&&localStorage.getItem('theme')){let a=localStorage.getItem('theme');a==='light'||a==='dark'||a==='black'?setTheme(a):window.matchMedia&&window.matchMedia('(prefers-color-scheme: dark)').matches?setTheme('dark'):setTheme('light')}else'auto'==='light'||'auto'==='dark'||'auto'==='black'?(setTheme('auto'),saveTheme('auto')):(saveTheme('auto'),window.matchMedia&&window.matchMedia('(prefers-color-scheme: dark)').matches?setTheme('dark'):setTheme('light'));let themeColorMeta=getMeta('theme-color');document.body.getAttribute('theme')!='light'&&(themeColorMeta.content='#000000')</script>
<div id=back-to-top></div>
<div id=mask></div><div class=wrapper><header class=desktop id=header-desktop>
<div class=header-wrapper>
<div class=header-title>
<a href=/ title="Something to Live"><span class=header-title-pre><i class="fas fa-kiwi-bird fa-fw"></i></span>Something to Live</a>
</div>
<div class=menu>
<div class=menu-inner><a class=menu-item href=/posts/ title=Posts> Posts </a><a class=menu-item href=/categories/reading/ title=Reading> Reading </a><a class=menu-item href=/tags/ title=Tags> Tags </a><a class=menu-item href=/series/ title="Reading Topics"> Reading Topics </a><a class=menu-item href=/categories/ title=Categories> Categories </a><a class=menu-item href=/about/ title="About Me"> About </a><a class=menu-item href=https://github.com/sosiristseng/ title=GitHub rel="noopener noreffer" target=_blank><i class="fab fa-github-alt fa-fw"></i> </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=Search id=search-input-desktop>
<a href=# onclick=return!1 class="search-button search-toggle" id=search-toggle-desktop title=Search>
<i class="fas fa-search fa-fw"></i>
</a>
<a href=# onclick=return!1 class="search-button search-clear" id=search-clear-desktop title=Clear>
<i class="fas fa-times-circle fa-fw"></i>
</a>
<span class="search-button search-loading" id=search-loading-desktop>
<i class="fas fa-spinner fa-fw fa-spin"></i>
</span>
</span><a href=# onclick=return!1 class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
</a></div>
</div>
</div>
</header><header class=mobile id=header-mobile>
<div class=header-container>
<div class=header-wrapper>
<div class=header-title>
<a href=/ title="Something to Live"><span class=header-title-pre><i class="fas fa-kiwi-bird fa-fw"></i></span>Something to Live</a>
</div>
<div class=menu-toggle id=menu-toggle-mobile>
<span></span><span></span><span></span>
</div>
</div>
<div class=menu id=menu-mobile><div class=search-wrapper>
<div class="search mobile" id=search-mobile>
<input type=text placeholder=Search id=search-input-mobile>
<a href=# onclick=return!1 class="search-button search-toggle" id=search-toggle-mobile title=Search>
<i class="fas fa-search fa-fw"></i>
</a>
<a href=# onclick=return!1 class="search-button search-clear" id=search-clear-mobile title=Clear>
<i class="fas fa-times-circle fa-fw"></i>
</a>
<span class="search-button search-loading" id=search-loading-mobile>
<i class="fas fa-spinner fa-fw fa-spin"></i>
</span>
</div>
<a href=# onclick=return!1 class=search-cancel id=search-cancel-mobile>
Cancel
</a>
</div><a class=menu-item href=/posts/ title=Posts>Posts</a><a class=menu-item href=/categories/reading/ title=Reading>Reading</a><a class=menu-item href=/tags/ title=Tags>Tags</a><a class=menu-item href=/series/ title="Reading Topics">Reading Topics</a><a class=menu-item href=/categories/ title=Categories>Categories</a><a class=menu-item href=/about/ title="About Me">About</a><a class=menu-item href=https://github.com/sosiristseng/ title=GitHub rel="noopener noreffer" target=_blank><i class="fab fa-github-alt fa-fw"></i></a><a href=# onclick=return!1 class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i>
</a></div>
</div>
</header>
<div class="search-dropdown desktop">
<div id=search-dropdown-desktop></div>
</div>
<div class="search-dropdown mobile">
<div id=search-dropdown-mobile></div>
</div>
<main class=main>
<div class=container><div class=toc id=toc-auto>
<h2 class=toc-title>Contents</h2>
<div class=toc-content id=toc-content-auto></div>
</div><script>document.getElementsByTagName("main")[0].setAttribute("pageStyle","wide")</script><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC","true")</script><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Computational Cognitive Neuroscience</h1><div class=post-meta>
<div class=post-meta-line>
<span class=post-author><i class="author fas fa-user-circle fa-fw"></i><a href=https://sosiristseng.github.io/ title=Author target=_blank rel="noopener noreffer author" class=author>Wen-Wei Tseng</a>
</span>&nbsp;<span class=post-category>included in </span>&nbsp;<span class=post-category>category <a href=/categories/course-notes/><i class="far fa-folder fa-fw"></i>Course Notes</a></span></div>
<div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2021-06-18>2021-06-18</time>&nbsp;<i class="far fa-edit fa-fw"></i>&nbsp;<time datetime=2021-06-18>2021-06-18</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;4121 words&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;9 minutes&nbsp;</div>
</div><div class="details toc" id=toc-static kept>
<div class="details-summary toc-title">
<span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span>
</div>
<div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents>
<ul>
<li><a href=#course-information>Course Information</a></li>
<li><a href=#central-questions>Central Questions</a>
<ul>
<li><a href=#cognitive-psychology>Cognitive Psychology</a></li>
<li><a href=#artificial-intelligence>Artificial intelligence</a></li>
<li><a href=#biological-plausibility>Biological plausibility</a></li>
<li><a href=#levels-scales-of-nervous-system>Levels (scales) of nervous system</a></li>
</ul>
</li>
<li><a href=#builidng-a-brain-with-math-models>Builidng a brain with math models</a>
<ul>
<li><a href=#3d-brain-structure>3D brain structure</a></li>
<li><a href=#the-scale-of-brain-models>The scale of brain models</a></li>
<li><a href=#neuron-biology>Neuron biology</a></li>
<li><a href=#hodgkin-and-huxley-model-1952>Hodgkin and Huxley model (1952)</a>
<ul>
<li><a href=#derived-models>Derived models</a></li>
</ul>
</li>
<li><a href=#nef-neural-engineering-network--spa-sementic-pointer-architecture>NEF (Neural Engineering Network) & SPA (Sementic Pointer Architecture)</a>
<ul>
<li><a href=#sementic-pointer>Sementic Pointer</a></li>
<li><a href=#embodieed-semantics>Embodieed semantics</a></li>
<li><a href=#working-memory>Working memory</a></li>
<li><a href=#spike-timing-dependent-plasticity-stdp>Spike-Timing-Dependent plasticity (STDP)</a></li>
</ul>
</li>
<li><a href=#spiking-models>Spiking models</a>
<ul>
<li><a href=#neural-responses>Neural responses</a></li>
<li><a href=#tuning-curve>Tuning curve</a></li>
<li><a href=#poisson-process-for-spike-firing>Poisson process for spike firing</a></li>
<li><a href=#rate-code-vs-temporal-code>Rate code v.s. temporal code</a></li>
</ul>
</li>
<li><a href=#encoding--decoding>Encoding / decoding</a></li>
</ul>
</li>
<li><a href=#neural-physiology>Neural Physiology</a>
<ul>
<li><a href=#excitable-membrane>Excitable membrane</a></li>
<li><a href=#action-potential>Action potential</a></li>
<li><a href=#neurotransmitters>Neurotransmitters</a></li>
</ul>
</li>
<li><a href=#neural-models>Neural models</a>
<ul>
<li><a href=#electrical-activity-of-neurons>Electrical activity of neurons</a></li>
<li><a href=#hh-model>HH model</a></li>
<li><a href=#considerations>Considerations</a></li>
</ul>
</li>
<li><a href=#dynamic-system-theory>Dynamic system theory</a>
<ul>
<li><a href=#morris-lecar-neuron-model>Morris-Lecar neuron model</a>
<ul>
<li><a href=#phase-plane-analysis>Phase plane analysis</a></li>
</ul>
</li>
<li><a href=#integrate-and-fire-if-model>Integrate and fire (IF) model</a></li>
<li><a href=#izhikevich-model>Izhikevich model</a></li>
<li><a href=#compartment-model>Compartment model</a></li>
</ul>
</li>
<li><a href=#filters>Filters</a></li>
<li><a href=#synapse-model>Synapse model</a></li>
<li><a href=#intro-to-brain>Intro to brain</a>
<ul>
<li><a href=#prerequisite>Prerequisite</a></li>
<li><a href=#reverse-enginering-the-brain>Reverse enginering the brain</a></li>
<li><a href=#why-a-brain>Why a brain</a></li>
<li><a href=#design-constraints>Design constraints</a></li>
<li><a href=#evolution-of-the-brain-in-cordates>Evolution of the brain in Cordates</a></li>
<li><a href=#central-pattern-generator>Central pattern generator</a></li>
</ul>
</li>
<li><a href=#nengo-programming>nengo programming</a>
<ul>
<li><a href=#classes>Classes</a></li>
<li><a href=#integrator-implementation>Integrator implementation</a></li>
<li><a href=#oscillator-implementation>Oscillator implementation</a></li>
</ul>
</li>
<li><a href=#connectivity-analysis>Connectivity analysis</a>
<ul>
<li><a href=#microscale-vs-macroscale>Microscale vs Macroscale</a></li>
<li><a href=#graph-theory>Graph theory</a></li>
<li><a href=#types-of-networks>Types of networks</a>
<ul>
<li><a href=#random>Random</a></li>
<li><a href=#scale-free>Scale-free</a></li>
<li><a href=#regular>Regular</a></li>
<li><a href=#modular>Modular</a></li>
<li><a href=#small-world>Small world</a></li>
</ul>
</li>
</ul>
</li>
<li><a href=#neural-engineering-framework-nef>Neural Engineering Framework (NEF)</a>
<ul>
<li><a href=#central-problems>Central problems</a></li>
<li><a href=#heterogeneity-in-realistic-neurla-networks>Heterogeneity in realistic neurla networks</a></li>
<li><a href=#building-nef-models-with-nengo>Building NEF models with nengo</a></li>
<li><a href=#central-nef-principles>Central NEF principles</a>
<ul>
<li><a href=#representation>Representation</a></li>
<li><a href=#transformation-of-encoding-information-by-neuron-clusters>Transformation of encoding information by neuron clusters</a></li>
<li><a href=#neual-dynamics-for-an-ensemble-of-neurons>Neual dynamics for an ensemble of neurons</a></li>
<li><a href=#ps>PS</a></li>
</ul>
</li>
</ul>
</li>
<li><a href=#neuro-representation>Neuro representation</a>
<ul>
<li><a href=#encoding--decoding-1>Encoding / decoding</a></li>
<li><a href=#symbols-used-when-neural-coding>Symbols used when neural coding</a></li>
<li><a href=#populational-encoding>Populational encoding</a>
<ul>
<li><a href=#some-linear-algebra>Some linear algebra</a></li>
<li><a href=#optimal-ensemble-linear-encoder>Optimal ensemble linear encoder</a></li>
</ul>
</li>
<li><a href=#example-horizontal-eye-position-in-nef>Example: horizontal eye position in NEF</a></li>
<li><a href=#vector-encoding--decoding>Vector encoding / decoding</a></li>
</ul>
</li>
<li><a href=#nengo-examples>Nengo examples</a></li>
<li><a href=#neural-transformation>Neural transformation</a>
<ul>
<li><a href=#multiplication>Multiplication</a></li>
<li><a href=#communication-channel>Communication channel</a></li>
<li><a href=#static-gain-c-multiplication-with-a-scalar>Static gain <code>c</code> (multiplication with a scalar)</a></li>
<li><a href=#addition>Addition</a></li>
<li><a href=#nonlinear-transformation>Nonlinear transformation</a></li>
<li><a href=#negative-weight>Negative weight</a></li>
</ul>
</li>
<li><a href=#neural-dynamics>Neural dynamics</a>
<ul>
<li><a href=#representation-1>Representation</a></li>
<li><a href=#linear-control-theory>Linear control theory</a></li>
<li><a href=#frequency-response-and-stability-analysis>Frequency response and stability analysis</a></li>
<li><a href=#neural-population-model>Neural population model</a></li>
<li><a href=#recurrent-connections>Recurrent connections</a></li>
<li><a href=#equations-for-different-levels>Equations for different levels</a></li>
</ul>
</li>
<li><a href=#sensation-and-perception>Sensation and Perception</a>
<ul>
<li><a href=#perception>Perception</a></li>
<li><a href=#psychophysica>Psychophysica</a></li>
</ul>
</li>
<li><a href=#vision>Vision</a>
<ul>
<li><a href=#v1-primary-visual-cortex>V1: primary visual cortex</a></li>
<li><a href=#successively-richer-layers>Successively richer layers</a></li>
<li><a href=#ventral-track>Ventral track</a></li>
<li><a href=#dorsal-track>Dorsal track</a></li>
<li><a href=#ambiguous-figures--optical-illusions>Ambiguous figures / optical illusions</a></li>
<li><a href=#feedback>Feedback</a></li>
<li><a href=#object-perception>Object perception</a></li>
</ul>
</li>
<li><a href=#autoencoders>Autoencoders</a>
<ul>
<li><a href=#ewerts-central-problems>Ewert&rsquo;s central problems</a></li>
<li><a href=#autoencoder-in-traditional-anns>Autoencoder in traditional ANNs</a></li>
<li><a href=#basic-machine-learning>Basic machine learning</a></li>
<li><a href=#classical-cognitive-systems-expert-system>Classical cognitive systems (expert system)</a></li>
<li><a href=#semantic-pointer-and-spa>Semantic pointer and SPA</a></li>
<li><a href=#encoding-information-in-the-semantic-pointer>Encoding information in the semantic pointer</a></li>
</ul>
</li>
<li><a href=#action-control>Action control</a>
<ul>
<li><a href=#affordance-competition-hypothesis>Affordance competition hypothesis</a></li>
<li><a href=#neural-optimal-control-hierachy-noch>Neural optimal control hierachy (NOCH)</a></li>
<li><a href=#performing-movement-in-robot-arms>Performing movement in robot arms</a></li>
<li><a href=#functional-level-model>Functional level model</a></li>
<li><a href=#rules-for-manipulation>Rules for manipulation</a></li>
<li><a href=#attention>Attention</a></li>
<li><a href=#tower-of-hanoi-task>Tower of Hanoi task</a></li>
<li><a href=#act-r-architecture>ACT-R architecture</a></li>
</ul>
</li>
<li><a href=#learning-and-memory>Learning and memory</a>
<ul>
<li><a href=#learning-in-biology>Learning in biology</a></li>
<li><a href=#machine-learning>Machine learning</a></li>
<li><a href=#biological-memories-in-detail>Biological memories in detail</a></li>
<li><a href=#conditioning>Conditioning</a>
<ul>
<li><a href=#terms>Terms</a></li>
<li><a href=#hippocampus>Hippocampus</a></li>
<li><a href=#inside-ltp--ltd>Inside LTP / LTD</a></li>
</ul>
</li>
<li><a href=#learning-rules>Learning rules</a>
<ul>
<li><a href=#hebbian>Hebbian</a></li>
<li><a href=#stdp>STDP</a></li>
<li><a href=#hpes-rule>hPES rule</a></li>
</ul>
</li>
<li><a href=#reinforcement-learning>Reinforcement learning</a>
<ul>
<li><a href=#value>Value</a></li>
<li><a href=#value-function-vs-and-prediction-error>Value function V(s) and prediction error</a></li>
<li><a href=#biological-rl>Biological RL</a></li>
<li><a href=#decision-making>Decision making</a></li>
</ul>
</li>
</ul>
</li>
<li><a href=#spaun-model>SPAUN model</a></li>
</ul>
</nav></div>
</div><div class=content id=content><p>Course Notes of Computational Cognitive Neuroscience by Prof. 鄭士康.</p>
<h2 id=course-information>Course Information</h2>
<ul>
<li>Lecturer: 鄭士康</li>
<li>Time: Fri. 789</li>
<li>Location: EE2-146</li>
<li>Homeworks:
<ul>
<li>HW1: 10/25 (Topic free)</li>
<li>HW2: 11/29</li>
<li>Group presentation: 1/22</li>
</ul>
</li>
<li>Handouts (G suite): <a href=https://drive.google.com/drive/u/1/folders/1GOcrwV9lR5Xk4bqPCXhxLc600DXRYl4K target=_blank rel="noopener noreffer">https://drive.google.com/drive/u/1/folders/1GOcrwV9lR5Xk4bqPCXhxLc600DXRYl4K</a></li>
</ul>
<h2 id=central-questions>Central Questions</h2>
<ul>
<li>Could machineㄋ perceive and think like humans?</li>
<li>Turing test</li>
<li>Stimuli -> acquire -> store -> transform (process, emotion) -> recall -> response (actions)</li>
</ul>
<h3 id=cognitive-psychology>Cognitive Psychology</h3>
<ul>
<li>Assumption: materialism: mind = brain function</li>
<li>Later became Cognitive Neuroscience</li>
<li>Models: Box and arrow -> Computational (mechanistic) vs Statistical model
<ul>
<li>Neuronal network <em>connections</em></li>
</ul>
</li>
</ul>
<h3 id=artificial-intelligence>Artificial intelligence</h3>
<ul>
<li>Reductionism</li>
<li>Search space of parameters</li>
<li>General probelm solver</li>
<li>Expert systems (symbol and rule-based)
<ul>
<li>Symbol processing ≢intellegence (Chinese room argument)</li>
<li>Does the machine really know semantics from the symbols and rules?</li>
</ul>
</li>
<li>Mimicking biological neuronetworks (H&H neuron model) -> spiking neuron network & Hebbian learning</li>
<li>Perceptron : Limitations by Minsky (unable to solve XOR probelm) -> 1st winter of AI</li>
<li>Multilayer and backpropagation: connectionism
<ul>
<li>Parallel distributed processing (1986): actually neuronetworks (a <em>taboo</em> by then)</li>
</ul>
</li>
<li>Convolutional neuronal networks (CNNs)
<ul>
<li>Computer vision</li>
<li>Simlimar to image processing in the visual cortex</li>
<li>Decomposition of features: stripes, angles, colors, etc.</li>
<li>Does intelligence <em>emerge</em> from complex networks?</li>
</ul>
</li>
<li>Dynamicism
<ul>
<li>embodied approach</li>
<li>Feedback system</li>
<li>Systems of non-linear DEs</li>
</ul>
</li>
<li>Cybernetics: control system for ML (system identification)</li>
<li>Bayeian approach : pure statistics regradless of underlying mechanism</li>
</ul>
<h3 id=biological-plausibility>Biological plausibility</h3>
<ul>
<li>Low = little similarity to biological counterpart
<ul>
<li>e.g. expert systems</li>
</ul>
</li>
<li>CNN: medium BP</li>
<li>SpiNNator and Nengo: high BP</li>
</ul>
<h3 id=levels-scales-of-nervous-system>Levels (scales) of nervous system</h3>
<ul>
<li>Focused on mesoscopic scale (neurons and synapses) in this course</li>
</ul>
<h2 id=builidng-a-brain-with-math-models>Builidng a brain with math models</h2>
<p>Why?</p>
<blockquote>
<p>Feymann: What I canot create, I do not understand.</p>
<ol>
<li>Understanding brain functions -> health (AD, PD, HD)</li>
<li>AI modeling and applications</li>
</ol>
</blockquote>
<h3 id=3d-brain-structure>3D brain structure</h3>
<p><a href=http://www.g2conline.org target=_blank rel="noopener noreffer">www.g2conline.org</a></p>
<h3 id=the-scale-of-brain-models>The scale of brain models</h3>
<ul>
<li>Neuron</li>
<li>Small clusters of neurons</li>
<li>Large scale connections (connectomes)</li>
</ul>
<h3 id=neuron-biology>Neuron biology</h3>
<ul>
<li>dendrite</li>
<li>soma</li>
<li>axon and myelin sheath</li>
</ul>
<h3 id=hodgkin-and-huxley-model-1952>Hodgkin and Huxley model (1952)</h3>
<ul>
<li>Math model from recordings of squid giant axon</li>
<li>Action potential</li>
<li>Biophysically accurate, but harder to do numerical analysis</li>
<li>Chance and Design by Alan Hodgkin</li>
</ul>
<h4 id=derived-models>Derived models</h4>
<ul>
<li>Simpler models with action potentials and multiple inputs</li>
<li>Leaky, Integrated and Fire model (<strong>LIF</strong> model)</li>
<li>LEBRA: single equation for a neuron, no spatial components</li>
<li>Compartment model of dendrite, soma, and axon.
<ul>
<li>Delay effect (+)</li>
<li>Discretization of the partial differential eqiuation (PDE) model</li>
<li>Could <strong>Delayed Differential Eqautions (DDEs)</strong> used in this context?</li>
</ul>
</li>
<li>Data (from fMIR, DTI, &mldr;) rich and theory poor</li>
<li>Large-scale models (connectome)</li>
<li>Neuromorphic hardware</li>
</ul>
<h3 id=nef-neural-engineering-network--spa-sementic-pointer-architecture>NEF (Neural Engineering Network) & SPA (Sementic Pointer Architecture)</h3>
<h4 id=sementic-pointer>Sementic Pointer</h4>
<ul>
<li>
<p>Sementics important for both symbolic and NN models</p>
</li>
<li>
<p>Example : autoencoder</p>
<ul>
<li>Dimemsion reduction layer by layer (raw data -> symbols)</li>
<li>Similar to visual cortex and associative areas</li>
<li>Reverse the network to adjust the weights</li>
<li>Loss = predicted - input</li>
</ul>
</li>
<li>
<p><strong>Spaun model</strong>: Autoencoders to preocess multiple sensory inputs as well as motor functions and decision making (transformation, working memory, reward, selection).</p>
</li>
<li>
<p>Ewert&rsquo;s Question: How is neural activity coordinated, learned and controlled?</p>
<ul>
<li>Capturing semantics</li>
<li>Encoding syntactic structures</li>
<li>Controlling information flow?</li>
<li>Memory, learning?</li>
</ul>
</li>
</ul>
<h4 id=embodieed-semantics>Embodieed semantics</h4>
<ul>
<li>Neural <em>firing patterns</em></li>
<li>High dimensional vector symbolic architectures</li>
</ul>
<h4 id=working-memory>Working memory</h4>
<ul>
<li>7 +/- 2 items, with highest recall for the 1st and the last item</li>
</ul>
<h4 id=spike-timing-dependent-plasticity-stdp>Spike-Timing-Dependent plasticity (STDP)</h4>
<ul>
<li>non-linear mapping for <em>learning</em> through synapses</li>
</ul>
<h3 id=spiking-models>Spiking models</h3>
<ul>
<li>Keywords: <em>spike firing rate</em>, <em>tuning curves</em>, *Poisson models</li>
<li>Adrian&rsquo;s frog leg test: loading induced spikes in the sciatic nerve
<ol>
<li>Stereotyped signals = spikes</li>
<li>Firing rate is a function to stimuli</li>
<li>Fatigue (adaptation) over time</li>
</ol>
</li>
</ul>
<h4 id=neural-responses>Neural responses</h4>
<ul>
<li>
<p>Raster plot: dot = one spike. x: time; y: neuron id</p>
</li>
<li>
<p>Firing rate histogram: x: time; y: # of spikes</p>
</li>
<li>
<p>Neural signal response: with Dirac delta function (signal processing?)</p>
<p>$$
\rho(t) = \Sigma_{n=1}^N\delta(t - t_i)
$$</p>
</li>
<li>
<p>Indivisual spikes -> Firing rates (in Hz) with a windows (moving average)</p>
</li>
<li>
<p>Similar to pulse density modulation (PDM)</p>
</li>
</ul>
<h4 id=tuning-curve>Tuning curve</h4>
<ul>
<li>x: stimuli trait; y: response</li>
<li>e.g. visual cortical neuron response to line orientation</li>
<li>Present in both sensory and motor cortices</li>
</ul>
<h4 id=poisson-process-for-spike-firing>Poisson process for spike firing</h4>
<ul>
<li>Poisson process: a random process with constant rate (or average waiting time).</li>
<li>The probability <code>P</code> with <code>n</code> events fired in a period <code>T</code> given a firing rate <code>r</code> could be expressed by:</li>
</ul>
<p>$$
P_T[n] = \frac{(rT)^n}{n!}e^{-rT}
$$</p>
<h4 id=rate-code-vs-temporal-code>Rate code v.s. temporal code</h4>
<ul>
<li>Dense firing for the former, sparse firing for the latter</li>
<li>Population code (a group of neurons firing)</li>
</ul>
<h3 id=encoding--decoding>Encoding / decoding</h3>
<ul>
<li>encoding: stimuli $x(t)$ -> spikes $\delta (t-t_i)$</li>
<li>decoding: spikes $\delta (t-t_i)$ -> intepretation of stimuli $\hat x(t)$</li>
</ul>
<h2 id=neural-physiology>Neural Physiology</h2>
<ul>
<li>Neuron: dendrites, soma, axon</li>
<li>Synapses: neurotransmitter / electrical conduction
<ul>
<li>AP from axon => Graded potential in dendrite / soma</li>
<li>Temporal / spatial summation of graded potential: AP in axial hillock</li>
</ul>
</li>
</ul>
<h3 id=excitable-membrane>Excitable membrane</h3>
<ul>
<li>Phospholipid bilayer (plasma membrane) as barrier</li>
<li>Integral / peripheral proteins: ion carriers and channels</li>
<li>Selected permeability to ions: Na / K gradients</li>
</ul>
<h3 id=action-potential>Action potential</h3>
<ul>
<li>Voltage-gated Na channel: both positive and negative feedback (fast)</li>
<li>Voltage-gated K channel: negative feedback (slow)</li>
<li>Leaky chloride channel: helping maintaining resting potential (constant)</li>
<li>Refractory period (5 ms): avaialble Na fraction is too low for AP</li>
<li>Nodes of Ranvier and myelin sheath: accelerates AP conduction</li>
</ul>
<h3 id=neurotransmitters>Neurotransmitters</h3>
<ul>
<li>Signaling molecules in the synaptic cleft</li>
<li>AP -> Ca influx -> vesicle release -> receptor bindin -> graded potentials (EPSP/IPSP) -> recycle / degradation of neurotransmitters</li>
</ul>
<h2 id=neural-models>Neural models</h2>
<ul>
<li>Features to reproduce: Integrating input, AP spikes, refractory period</li>
</ul>
<h3 id=electrical-activity-of-neurons>Electrical activity of neurons</h3>
<ul>
<li><a href=https://en.wikipedia.org/wiki/Nernst_equation target=_blank rel="noopener noreffer">Nernst equation</a> for one species of ion across a semipermeable membrane</li>
<li><a href=https://en.wikipedia.org/wiki/Goldman_equation target=_blank rel="noopener noreffer">GHK voltage equation</a> for multiple ions</li>
<li>Quasi-ohmic assumption for ion channels $I_x = g_x (V_m-E_x)$</li>
<li>Membrane as capacitor (1 $\mu F/ cm^2$)</li>
<li>Equivalent circuit: An RC circuit</li>
</ul>
<h3 id=hh-model>HH model</h3>
<ul>
<li>GHK voltage equation not applicable (not in steady state)</li>
<li>Using Kirchhoff&rsquo;s current law to get voltage change over time</li>
<li>Parameters from experiments on the squid giant axon</li>
<li>K channel: gating variable n</li>
</ul>
<p>$$
\begin{aligned}
g_K &= \bar g_Kn^4 \cr
\frac{dn}{dt} &= \alpha - n (\alpha + \beta)
\end{aligned}
$$</p>
<pre><code>α and β are determined by voltage (membrane potential)
</code></pre>
<ul>
<li>
<p>Na channel: two gating variables, m and h</p>
<p>$$
\begin{aligned}
g_{Na} &= \bar g_{Na} m^3h \cr
\frac{dm}{dt} &= \alpha_m - n (\alpha_m + \beta_m) \cr
\frac{dh}{dt} &= \alpha_h - n (\alpha_h + \beta_h) \cr
\end{aligned}
$$</p>
<p><code>αs and βs are determined by voltage (membrane potential)</code></p>
</li>
</ul>
<h3 id=considerations>Considerations</h3>
<ul>
<li>Model fidelity (biological relevance) vs simplicity (ease to simulate and analyze)</li>
<li>Biological plausibility</li>
</ul>
<h2 id=dynamic-system-theory>Dynamic system theory</h2>
<p>A system of ODEs</p>
<p>e.g. Butterfly effect (chaos system): small deviation of initial conditions > huge different results</p>
<h3 id=morris-lecar-neuron-model>Morris-Lecar neuron model</h3>
<ul>
<li>Similar to the HH model (KCL)</li>
<li>Ca, K, and Cl ions</li>
<li>two state variables: voltage (V) and one variable (w) for K</li>
<li>using tanh and cosh functions</li>
</ul>
<h4 id=phase-plane-analysis>Phase plane analysis</h4>
<ul>
<li>Stability: Eigenvalues of rhs Jacobian matrix in the steady-state</li>
<li>External current (Ie) = 0: single stable steady-state (interscetion of V and w nullclines)</li>
<li>Increasing Ie: shifting V null cline => unstable steady-state (limit cycle)</li>
<li>Bifurcation: V vs Ie</li>
</ul>
<h3 id=integrate-and-fire-if-model>Integrate and fire (IF) model</h3>
<ul>
<li>A simple RC circuit</li>
<li>Single state variable (V)</li>
<li>Use of conditional statements to control spiking firing and refractory period</li>
<li>Used in nengo (plus leaky = LIF model)</li>
<li>Firing rate adaption: IF model + more terms</li>
</ul>
<h3 id=izhikevich-model>Izhikevich model</h3>
<ul>
<li>Two state variables</li>
<li>Realistic spike patterns by adjusting parameters</li>
<li>Could be used in large systems (100T synapses)</li>
</ul>
<h3 id=compartment-model>Compartment model</h3>
<ul>
<li>Spatial discretization for neuron models</li>
<li>Coupled RC circuits -> FEM grids</li>
</ul>
<h2 id=filters>Filters</h2>
<ul>
<li>Presynaptic AP -> synapse neurotransmitter release -> Postsynaptic potentials</li>
<li>Approximated by an LTI(linear, time invariant) system</li>
<li>Linear: superposition</li>
<li>Time invariant: unchanged with time shifting</li>
<li>Impulse response: given a impulse (delta function) -> h(t), transformed results</li>
<li>Convolution: h(t) instead of the system itself</li>
<li>Fourier transform: Convolution -> multiplication</li>
</ul>
<h2 id=synapse-model>Synapse model</h2>
<ul>
<li>Synapse = RC low pass filters with time scale = $\tau$</li>
<li>$\tau$ is dependent on types of neurotransmittera and receptors</li>
</ul>
<h2 id=intro-to-brain>Intro to brain</h2>
<h3 id=prerequisite>Prerequisite</h3>
<ul>
<li>Simple linear algebra (vector and matrix operations)</li>
<li>Graph theory: connections</li>
</ul>
<h3 id=reverse-enginering-the-brain>Reverse enginering the brain</h3>
<ul>
<li>engineeringchallenges.org</li>
<li>Complexity, scale, connection, plasticiy, low-power</li>
<li>Design: brain scheme; designer: natural selection</li>
</ul>
<h3 id=why-a-brain>Why a brain</h3>
<ul>
<li>To survive and thrive.</li>
<li>Brainless (single-celled organisms): simple preceptions and reactions. Some endogenous activity</li>
<li>Simple brain (C. elegans): aversive response and body movement
<ul>
<li>Connectome routing study (as in EDA) showed 90% of the neurons are in the optimal positions</li>
</ul>
</li>
<li>General scheme: sensory -> CNS -> motor (with endogenous states (thoughts) in the CNS)</li>
</ul>
<h3 id=design-constraints>Design constraints</h3>
<ul>
<li>Information theory (information efficiency)</li>
<li>Energy efficiency</li>
<li>Space efficiency</li>
<li>Human brain is already relatively larger than almost all animals</li>
</ul>
<h3 id=evolution-of-the-brain-in-cordates>Evolution of the brain in Cordates</h3>
<ul>
<li>Dorsal neural tube -> differentialtion respecting sensory,motor, and inter connections</li>
</ul>
<h3 id=central-pattern-generator>Central pattern generator</h3>
<ul>
<li>The brainless walking cat: endogenous activity in the spinal cord</li>
<li>Main functioN unit in the CNS</li>
</ul>
<h2 id=nengo-programming>nengo programming</h2>
<h3 id=classes>Classes</h3>
<ul>
<li>Network: model itself</li>
<li>Node: input signal</li>
<li>Ensemble: neuronss</li>
<li>Coonnection: synapses</li>
<li>Probe: output</li>
<li>Simulator: simulator (literally)</li>
</ul>
<h3 id=integrator-implementation>Integrator implementation</h3>
<ul>
<li>Similar to the Euler method in numerical integration</li>
</ul>
<p>$$
y[n] = A { y[n-1] + \Delta t x[n-1] }
$$</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
<span class=kn>import</span> <span class=nn>nengofrom</span> <span class=n>nengo</span><span class=o>.</span><span class=n>processes</span>
<span class=kn>import</span> <span class=nn>Piecewise</span>

<span class=c1># The model</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Network</span><span class=p>(</span><span class=n>label</span><span class=o>=</span><span class=s1>&#39;Integrator&#39;</span><span class=p>)</span>

<span class=k>with</span> <span class=n>model</span><span class=p>:</span>
    <span class=c1># Neurons representing one number</span>
    <span class=n>A</span> <span class=o>=</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Ensemble</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=n>dimensions</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>

    <span class=c1># Input signal</span>
    <span class=n>src</span> <span class=o>=</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Node</span><span class=p>(</span><span class=n>Piecewise</span><span class=p>({</span><span class=mi>0</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>:</span> <span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>4</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span><span class=mi>5</span><span class=p>:</span> <span class=mi>0</span><span class=p>}))</span>

    <span class=n>tau</span> <span class=o>=</span> <span class=mf>0.1</span>

    <span class=c1># Connect the population to itself</span>
    <span class=c1># transform: transformation matrix</span>
    <span class=c1># synapse: time scale of low pass filter</span>
    <span class=n>nengo</span><span class=o>.</span><span class=n>Connection</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>A</span><span class=p>,</span> <span class=n>transform</span><span class=o>=</span><span class=p>[[</span><span class=mi>1</span><span class=p>]],</span> <span class=n>synapse</span><span class=o>=</span><span class=n>tau</span><span class=p>)</span>
    <span class=n>nengo</span><span class=o>.</span><span class=n>Connection</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=n>A</span><span class=p>,</span> <span class=n>transform</span><span class=o>=</span><span class=p>[[</span><span class=n>tau</span><span class=p>]],</span> <span class=n>synapse</span><span class=o>=</span><span class=n>tau</span><span class=p>)</span>
    <span class=n>input_probe</span> <span class=o>=</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Probe</span><span class=p>(</span><span class=n>src</span><span class=p>)</span>
    <span class=n>A_probe</span> <span class=o>=</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Probe</span><span class=p>(</span><span class=n>A</span><span class=p>,</span> <span class=n>synapse</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>

<span class=c1># Create our simulator</span>
<span class=k>with</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Simulator</span><span class=p>(</span><span class=n>model</span><span class=p>)</span> <span class=k>as</span> <span class=n>sim</span><span class=p>:</span>
    <span class=c1># Run it for 6 seconds</span>
    <span class=n>sim</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=mi>6</span><span class=p>)</span>
<span class=c1># Plot the decoded output of the ensemble</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>sim</span><span class=o>.</span><span class=n>trange</span><span class=p>(),</span> <span class=n>sim</span><span class=o>.</span><span class=n>data</span><span class=p>[</span><span class=n>input_probe</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;Input&#34;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>sim</span><span class=o>.</span><span class=n>trange</span><span class=p>(),</span> <span class=n>sim</span><span class=o>.</span><span class=n>data</span><span class=p>[</span><span class=n>A_probe</span><span class=p>],</span> <span class=s1>&#39;k&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;Integrator output&#34;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>();</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></td></tr></table>
</div>
</div><h3 id=oscillator-implementation>Oscillator implementation</h3>
<p>Harmonic oscillator: one 2nd order ODE -> two 1st order ODEs</p>
<p>$$
\begin{aligned}
\frac{d^2x}{dt^2} &= -\omega^2 x \cr
\vec{x} &= \begin{bmatrix}x \cr \frac{dx}{dt} \end{bmatrix} \cr
\frac{d\vec{x}}{dt} &= \begin{bmatrix}0 & 1 \cr -\omega^2 & 1 \end{bmatrix} \vec{x} = A \vec{x}
\end{aligned}
$$</p>
<p><strong>nengo</strong>:</p>
<p>$$
\begin{aligned}
\vec{x} &= \begin{bmatrix}x_0 \cr x_1 \end{bmatrix} \cr
\vec{x}[n] &= \begin{bmatrix}1 & \Delta t \cr-\omega^2\Delta t & 1 \end{bmatrix} \cr \vec{x}[n-1] &= B \vec{x}[n-1] \cr
\end{aligned}
$$</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
<span class=kn>import</span> <span class=nn>nengo</span>
<span class=kn>from</span> <span class=nn>nengo.processes</span> <span class=kn>import</span> <span class=n>Piecewise</span>

<span class=c1># Create the model object</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Network</span><span class=p>(</span><span class=n>label</span><span class=o>=</span><span class=s1>&#39;Oscillator&#39;</span><span class=p>)</span>

<span class=k>with</span> <span class=n>model</span><span class=p>:</span>
    <span class=c1># Neurons representing 2 numbers (dim = 2)</span>
    <span class=n>neurons</span> <span class=o>=</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Ensemble</span><span class=p>(</span><span class=mi>200</span><span class=p>,</span> <span class=n>dimensions</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
    <span class=c1># Input signal</span>
    <span class=n>src</span> <span class=o>=</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Node</span><span class=p>(</span><span class=n>Piecewise</span><span class=p>({</span><span class=mi>0</span><span class=p>:</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=mf>0.1</span><span class=p>:</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>]}))</span>
    <span class=n>nengo</span><span class=o>.</span><span class=n>Connection</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=n>neurons</span><span class=p>)</span>
    <span class=c1># Create the feedback connection. Note the transformation matrix</span>
    <span class=n>nengo</span><span class=o>.</span><span class=n>Connection</span><span class=p>(</span><span class=n>neurons</span><span class=p>,</span> <span class=n>neurons</span><span class=p>,</span> <span class=n>transform</span><span class=o>=</span><span class=p>[[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]],</span> <span class=n>synapse</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>

    <span class=n>input_probe</span> <span class=o>=</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Probe</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=s1>&#39;output&#39;</span><span class=p>)</span>
    <span class=n>neuron_probe</span> <span class=o>=</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Probe</span><span class=p>(</span><span class=n>neurons</span><span class=p>,</span> <span class=s1>&#39;decoded_output&#39;</span><span class=p>,</span> <span class=n>synapse</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>

<span class=c1># Create the simulator</span>
<span class=k>with</span> <span class=n>nengo</span><span class=o>.</span><span class=n>Simulator</span><span class=p>(</span><span class=n>model</span><span class=p>)</span> <span class=k>as</span> <span class=n>sim</span><span class=p>:</span>
    <span class=c1># Run it for 5 seconds</span>
    <span class=n>sim</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span>

<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>sim</span><span class=o>.</span><span class=n>trange</span><span class=p>(),</span> <span class=n>sim</span><span class=o>.</span><span class=n>data</span><span class=p>[</span><span class=n>neuron_probe</span><span class=p>])</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Time (s)&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=s1>&#39;large&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>([</span><span class=s1>&#39;$x_0$&#39;</span><span class=p>,</span> <span class=s1>&#39;$x_1$&#39;</span><span class=p>])</span>

<span class=n>data</span> <span class=o>=</span> <span class=n>sim</span><span class=o>.</span><span class=n>data</span><span class=p>[</span><span class=n>neuron_probe</span><span class=p>]</span>
<span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>data</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>data</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Decoded Output&#39;</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;$x_0$&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>20</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;$x_1$&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>20</span><span class=p>)</span>
<span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
<span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</code></pre></td></tr></table>
</div>
</div><h2 id=connectivity-analysis>Connectivity analysis</h2>
<ul>
<li>Structural: anatomical structures e.g. water diffusion via DTI</li>
<li>Functional: statisitc, dynamic weights</li>
<li>Effective: causal interactions (presynaptic spikes -> postsynamptic firing)</li>
<li>ref. 因果革命</li>
</ul>
<h3 id=microscale-vs-macroscale>Microscale vs Macroscale</h3>
<ul>
<li>Microscale: um ~ nm (synapses)</li>
<li>Macroscale: mm (voxels) coherent regions</li>
</ul>
<h3 id=graph-theory>Graph theory</h3>
<ul>
<li>Node: brain areas (or neurons)</li>
<li>Edges: connections (or synapses)</li>
<li>Represented by adjacency matrices (values = connection weights)</li>
</ul>
<h3 id=types-of-networks>Types of networks</h3>
<ul>
<li>Nodes in a circle; Connections in an adjacency matrix</li>
<li>Measure: degrees of a node (inward / outward) / neighborhood (Modularity Q, Small-worldness S)</li>
</ul>
<h4 id=random>Random</h4>
<p>Same edge probability</p>
<h4 id=scale-free>Scale-free</h4>
<ul>
<li>Power law</li>
<li>Fractal</li>
<li>Increased robustness to neural damage</li>
</ul>
<h4 id=regular>Regular</h4>
<ul>
<li>Local connections only</li>
</ul>
<h4 id=modular>Modular</h4>
<ul>
<li>hierarchial clusters</li>
<li>Built by attraction and repulson between nodes</li>
<li>In some biological neural networks</li>
</ul>
<h4 id=small-world>Small world</h4>
<ul>
<li>Similar to social networks, sparse global connections</li>
<li>A few hubs (opinion leaders) with high degrees (connecting edges)</li>
<li>Rich hub organization in biological neural networks (10 times the connections to the average)</li>
<li>Anatomical basis (maximize space / energy efficiency)</li>
</ul>
<h2 id=neural-engineering-framework-nef>Neural Engineering Framework (NEF)</h2>
<ul>
<li>By Eliasmith</li>
<li>Intended for constant structures without synaptic plasticity
<ul>
<li>Compared to SNNs (with learning = synaptic plasticity)</li>
</ul>
</li>
<li>Nerual compiler (high level function &lt;=> low level spikes)</li>
</ul>
<h3 id=central-problems>Central problems</h3>
<ul>
<li>Stimuli detection (sensors)</li>
<li>Representation / manipulation of information (sensory n.)
<ul>
<li>As spikes (pulse density modulation = PDM)</li>
</ul>
</li>
<li>Recall / transform (CNS)</li>
</ul>
<h3 id=heterogeneity-in-realistic-neurla-networks>Heterogeneity in realistic neurla networks</h3>
<ul>
<li>Different set of parameters for each neuron in response to stimuli</li>
<li>Represented as <em>tuning curves</em></li>
</ul>
<h3 id=building-nef-models-with-nengo>Building NEF models with nengo</h3>
<ul>
<li>Hypothesis / data / structure from the real counterpart</li>
<li>Build NEF and check behavior</li>
<li>Rinse and repeat</li>
</ul>
<h3 id=central-nef-principles>Central NEF principles</h3>
<h4 id=representation>Representation</h4>
<ul>
<li>Action potential: digital, non-linear encoding (axon hillock)</li>
<li>Graded potential: analog, linear decoding (dendrite)</li>
<li>Compared to ANNs:
<ul>
<li>dendrite = wieghted sum from other neurons</li>
<li>axon hillock: non-linear activation function (real number output)</li>
</ul>
</li>
<li>Examples: Physical values: heat, light, velocity, position
<ul>
<li>mimicking sensory neurons = transducer producing pulse signals</li>
</ul>
</li>
</ul>
<h4 id=transformation-of-encoding-information-by-neuron-clusters>Transformation of encoding information by neuron clusters</h4>
<h4 id=neual-dynamics-for-an-ensemble-of-neurons>Neual dynamics for an ensemble of neurons</h4>
<p>HH mdoel, LIF, control theory</p>
<h4 id=ps>PS</h4>
<ul>
<li>Neurons are noisy</li>
<li>In the NEF: the basic unit is an ensemble of neurons</li>
<li>Post synaptic current: approximated by one time constant</li>
</ul>
<h2 id=neuro-representation>Neuro representation</h2>
<h3 id=encoding--decoding-1>Encoding / decoding</h3>
<ul>
<li>Ensemble = Digital-analog converter like digital audio processing</li>
</ul>
<h3 id=symbols-used-when-neural-coding>Symbols used when neural coding</h3>
<ul>
<li>x: strength of external stimuli</li>
<li>J(x): x-induced current</li>
<li>$a(x) = G[J(x)]$: firing rate of spikes ≈ activation function in ANNs</li>
<li>Most important parameters
<ul>
<li>$J_{th}$ (threshold current)</li>
<li>$\tau_{ref}$ (refractory period → maximal spiking rate)</li>
</ul>
</li>
</ul>
<h3 id=populational-encoding>Populational encoding</h3>
<p>A group of neurons determine the value by their spikes collectively.
Contrary to <em>sparse coding</em>.</p>
<h4 id=some-linear-algebra>Some linear algebra</h4>
<ul>
<li>Any vector couldbe decomposed as an unique linear cmobination of basis vectors</li>
<li>The most convienent ones are orthogonal bases e.g. sin / cos in Fourier series</li>
<li>The stimuli through the ensemble could be estimated from the linear combination of wieghts of neurons with different tuning curves</li>
<li>Simpleset : two neuron model (on and off)</li>
<li>Adding more and more neurons differing in tuning curves (more bases) = more accurate representation</li>
</ul>
<h4 id=optimal-ensemble-linear-encoder>Optimal ensemble linear encoder</h4>
<ul>
<li>Calculated by solving a linear system</li>
<li>Nengo derives the best set of weights for an ensemble of neurons automatically</li>
<li>Adding Gaussian noise in fact enhanced the robustness of the matrix of tuning cuves</li>
</ul>
<h3 id=example-horizontal-eye-position-in-nef>Example: horizontal eye position in NEF</h3>
<ul>
<li>System description
<ul>
<li>Max firing rate = 300 Hz</li>
<li>On-off neurons</li>
<li>Goal: linear tuning curve</li>
</ul>
</li>
<li>How neurons work in abducens motor neuron: an integrator</li>
<li>Populations, noise, and constraints</li>
<li>Solution errors associated to the number of neurons
<ul>
<li>Noise error</li>
<li>Static error</li>
<li>Rounding error</li>
</ul>
</li>
</ul>
<h3 id=vector-encoding--decoding>Vector encoding / decoding</h3>
<ul>
<li>Similar to the scalar case, but replaced with vectors</li>
<li>Automatically handled by the nengo framework</li>
</ul>
<h2 id=nengo-examples>Nengo examples</h2>
<ul>
<li><code>RateEncoding.py</code></li>
<li><code>ArmMovement2D.py</code></li>
</ul>
<h2 id=neural-transformation>Neural transformation</h2>
<ul>
<li>Linear</li>
<li>Non-linear</li>
<li>Weighting: positive (excitatory) / negative (inhibitory)</li>
</ul>
<h3 id=multiplication>Multiplication</h3>
<ul>
<li>Controlled integrator (memory)</li>
<li>ref: <code>Multiplication.py</code></li>
<li>Traditional ANN counterpart: Neural clusters A and B fully connected to combination layer, respectively</li>
<li>Making a subnetwork: factory function</li>
</ul>
<h3 id=communication-channel>Communication channel</h3>
<ul>
<li>Output of one ensemble => Input of another ensemble</li>
<li>Traditional ANN counterpart: fully-connected layers</li>
<li>$w_{ji} = \alpha_je_jd_i$</li>
<li>nengo: simply <code>Connection(A, B)</code></li>
</ul>
<h3 id=static-gain-c-multiplication-with-a-scalar>Static gain <code>c</code> (multiplication with a scalar)</h3>
<ul>
<li>$w_{ji} = c\alpha_je_jd_i$</li>
<li>nengo: <code>Connection(A, B, transform=c)</code></li>
</ul>
<h3 id=addition>Addition</h3>
<ul>
<li>c = a + b</li>
<li>nengo: <code>Connection(A, C); Connection(B, C)</code></li>
<li>Adding two vectors: just change <code>dimesion</code></li>
</ul>
<h3 id=nonlinear-transformation>Nonlinear transformation</h3>
<ul>
<li>nengo: define a vector transformation functon <code>f</code> => <code>Connection(A, B, function=f)</code></li>
</ul>
<h3 id=negative-weight>Negative weight</h3>
<ul>
<li>An ensemble of inhibitory neurons</li>
</ul>
<h2 id=neural-dynamics>Neural dynamics</h2>
<ul>
<li>Neural control systems: non-linear, time-variant (modern control theory)</li>
</ul>
<h3 id=representation-1>Representation</h3>
<ul>
<li>1st order ODEs</li>
<li>State variables as a vector</li>
<li>$\mathbf{x}(t) = \mathbf{x}(t - \Delta t) + f(t - \Delta t, \mathbf{x}(t - \Delta t))$</li>
<li>Example: cellular automata finite state machine (Game of life)</li>
</ul>
<h3 id=linear-control-theory>Linear control theory</h3>
<p>u: input, y: output, x: internal states
$\mathbf{\dot{x}}(t) = A \mathbf{\dot{x}}(t) + B \mathbf{u}(t)$
$\mathbf{y}(t) = C \mathbf{x}(t) + D \mathbf{u}(t)$</p>
<h3 id=frequency-response-and-stability-analysis>Frequency response and stability analysis</h3>
<ul>
<li>Laplace transform $L{f(t)} = \int^\infty_0e^{-st}f(t)dt = F(s)$</li>
<li>Impulse response: $h(t) = \frac{1}{\tau}e^{-t/\tau}, \ H(s) = \frac{1}{1 + s\tau}$. Stable (pole at the left half plane)</li>
<li>Convolution in the time domain = multiplication in the Laplace (s-domain)</li>
</ul>
<h3 id=neural-population-model>Neural population model</h3>
<ul>
<li>Linear decoder for post-synaptic current (PSC)
<ul>
<li>$A^\prime = \tau A + I$</li>
<li>$B^\prime = \tau B$</li>
</ul>
</li>
</ul>
<h3 id=recurrent-connections>Recurrent connections</h3>
<ul>
<li>Positive feedback: <code>Feedback1.py</code></li>
<li>Negative feedback: <code>Feedback2.py</code> (without stimuli), <code>Feedback3.py</code> (with stimuli)</li>
<li>Dynamics: <code>Dynamics1.py</code> and <code>Dynamics2.py</code>: step stimuli + feedback</li>
<li>Integrators: $A = \frac{-1}{\tau} I$</li>
<li>Oscillators: $A = \begin{bmatrix} 0&1 cr\ -\omega^2&0 \cr \end{bmatrix}$</li>
</ul>
<h3 id=equations-for-different-levels>Equations for different levels</h3>
<ul>
<li>Nengo: higher level</li>
<li>Implementation: lower rate / spiking levels</li>
</ul>
<h2 id=sensation-and-perception>Sensation and Perception</h2>
<p>Environment (stimulation) (analog signal) -> sensory transduction (feature extraction) -> impulse signal (sensory nerve) -> perceptions (sensory cortex) -> processing (CNS) -> action selection (motor cortex) -> impulse signal (motor nerve) -> acuator(e.g. muscle) -> action</p>
<h3 id=perception>Perception</h3>
<ul>
<li>Internal representation of stimuli impulses</li>
<li>The experience in the association cortex (not necessary the same as the outside world)</li>
<li>Book: making uo the mind</li>
</ul>
<h3 id=psychophysica>Psychophysica</h3>
<p>e.g. Psychoacoustics: used in MP3 compression</p>
<ul>
<li>Threshold in quiet / noisy environment</li>
<li>Equal-loudness contour in different frequencies</li>
<li>Weber&rsquo;s law: change perceived in percent change $S = klg\frac{I}{I_0}$</li>
</ul>
<h2 id=vision>Vision</h2>
<ul>
<li>Convergence of information inside retina
<ul>
<li>260M photoreceptor cells indirectly connected to 2M ganglion (optic nerve) cells)</li>
<li>Dimension reduction (pooling / convolution)</li>
</ul>
</li>
<li>Need of learning to see (mechanism of amblyopia): Neural wiring in the visual tract and the visual cortex (training of CNNs)</li>
</ul>
<h3 id=v1-primary-visual-cortex>V1: primary visual cortex</h3>
<ul>
<li>Detection of oriented edges, grouped by cortical columns with sensitivity to different angles</li>
<li>Similar to the tuning curve in NEF</li>
</ul>
<h3 id=successively-richer-layers>Successively richer layers</h3>
<p>Optic nerve -> LGN (thalamus) -> V1 -> V2 / V4 -> dorsal (metric) or ventral (identification) tracks</p>
<ul>
<li>Feature extraction</li>
<li>Similar to convolutional neural network (CNNs)
<ul>
<li>Demonstrated in fMRI</li>
</ul>
</li>
</ul>
<h3 id=ventral-track>Ventral track</h3>
<ul>
<li><strong>What</strong> is the object?</li>
<li>V2 / V4 -> Post. Inf. temporal (PIT) cortex -> Ant. Inf. temporal (AIT) cortex</li>
<li>PIT: More complex features e.g. fusiform face area for fast facial recognition</li>
<li>AIT: Classification of objects regardless of size, color, viewing angle&mldr;
<ul>
<li>Hyperdimensional vector (EECS) = semantic pointer (NEF)</li>
<li>Neural emsemble of 20000 in monkeys</li>
</ul>
</li>
<li>Thus the functions of the temporal lobe = categorizing the world:
<ul>
<li>Primary and associative auditory</li>
<li>Labeling visual objects</li>
<li>Language processing for both visual and auditory cues</li>
<li>Episodic memory formation by hippocampus</li>
</ul>
</li>
</ul>
<h3 id=dorsal-track>Dorsal track</h3>
<ul>
<li><strong>Where</strong> is the object?</li>
<li>V1 -> V2 -> V5 -> parietal lobe (visual association area)</li>
<li>metrical information and mathematics</li>
<li>Motion detection and information for further actions</li>
</ul>
<h3 id=ambiguous-figures--optical-illusions>Ambiguous figures / optical illusions</h3>
<p>Forms 2 attractors (intepretations)</p>
<p>e.g Necker cube</p>
<h3 id=feedback>Feedback</h3>
<ul>
<li>External cue and expectation (top down perception)</li>
<li>Report to LGN about the error</li>
</ul>
<h3 id=object-perception>Object perception</h3>
<ul>
<li>In biology: robust recognition despite color, viewing angle differences (object consistency)</li>
<li>View-dependent frame of reference vs. View-invariant (grammer pattern) frame of reference</li>
</ul>
<h2 id=autoencoders>Autoencoders</h2>
<h3 id=ewerts-central-problems>Ewert&rsquo;s central problems</h3>
<ul>
<li>Preception: encoding stimuli from analog to digital spikes</li>
<li>Central processing: transformation and recall of information, action selection</li>
<li>Action execution: decoding digital spikes to response</li>
</ul>
<h3 id=autoencoder-in-traditional-anns>Autoencoder in traditional ANNs</h3>
<ul>
<li>Compressing the input into a smaller (dim.) representation then expand to the estimation
<ul>
<li>Hyper dimension vector in CS</li>
<li>Semantic pointer in NEF</li>
</ul>
</li>
<li>Novelty detection: comparison of the input to the output from trained autoencoder</li>
</ul>
<h3 id=basic-machine-learning>Basic machine learning</h3>
<ul>
<li>For y = f(x), find f</li>
<li>Training, testing, validation sets</li>
<li>Learning curves: overfitting if overtraining</li>
<li>Cross validation to reduce overfitting and increase testing accuracy
<ul>
<li>K-fold cross validation</li>
</ul>
</li>
<li>SVM: once worked better than ANNs
<ul>
<li>Converting low dim but complex border to higer dim. simpler (even linear) border by trasnformation of data points</li>
</ul>
</li>
</ul>
<h3 id=classical-cognitive-systems-expert-system>Classical cognitive systems (expert system)</h3>
<ul>
<li>Symbols and syntax processing (LISP)</li>
<li>Failed due to low BP (unable to solve to meaning of symbols)</li>
<li>Another attempt: connectionist (semantic space) => too complex</li>
<li>Symbol binding system: 500M neurons to recognize simple sentences (fail)</li>
<li>Until the semantic pointer hypothesis: explaining high level cognitive function
<ul>
<li>Halle Berry neurons (grandmother neurons): highly selective to one category instances (sparse coding)</li>
<li>However most instances are population coding</li>
</ul>
</li>
</ul>
<h3 id=semantic-pointer-and-spa>Semantic pointer and SPA</h3>
<ul>
<li>Equals to hyperdimensional vector in the mathematical sense</li>
<li>Presented by an ensemble of neurons in biology</li>
<li>The semantic space (hyperdimensional space) holds information features
<ul>
<li>Needs enough dimesions for the overwhelming number of concepts in the world</li>
</ul>
</li>
<li>Pointers = symbols = general concepts
<ul>
<li>Indirect addressing of complex information</li>
<li>Shallow and deep manipulation (dual coding theory)</li>
<li>Efficient transformation (call by address)</li>
</ul>
</li>
<li>Shallow semantics (e.g. text mining): symbols and stats only, does not encode the meaning of words</li>
<li>Nengo: <code>nengo-spa</code></li>
</ul>
<h3 id=encoding-information-in-the-semantic-pointer>Encoding information in the semantic pointer</h3>
<p>Circular convolution for syntax processing</p>
<ul>
<li>Readily extract the information in SP after filtered some noise</li>
<li>Does not incur extra dimensions</li>
<li>Works on reals numbers (XOR works on binaries only)</li>
<li>Solves Jackendoff&rsquo;s challenges
<ul>
<li>Binding problem : red + square vs green + circle</li>
<li>Problem of 2: small star vs big star</li>
<li>Problem of variable: blue fly (n.) vs. blue fly(v.): binding restrictions</li>
<li>Binding in working memory vs long-term memory</li>
</ul>
</li>
</ul>
<p>One could coombine multiple sources of input (word, visual, smell, auditory)</p>
<h2 id=action-control>Action control</h2>
<p>Behavioral pattern / coordination</p>
<h3 id=affordance-competition-hypothesis>Affordance competition hypothesis</h3>
<ul>
<li>Affordance part: continously updating the status</li>
<li>Competition part: select best action by utility (spiking activity)
In biology:</li>
<li>Premotor / supplementary motor cortex
<ul>
<li>Weighted summation of previously learned motor components (basis functions) -> desired movement</li>
</ul>
</li>
<li>Primary motor cortex</li>
<li>Basal ganglia
<ul>
<li>Caudate, putamen, globus pallidus, SN</li>
<li>Excitation and inhibitory projections</li>
<li>Dopaminergic neurons: reward expectation: reinforcement learning</li>
<li>Movement initiation</li>
<li>Direct, indirect, and hyperdirect pathways</li>
</ul>
</li>
<li>Cerebellum
<ul>
<li>Learning and control of movements</li>
<li>Error-driven (similar to back propagation): supervised learning</li>
</ul>
</li>
<li>Hippocampus: self-organizing (Hebbian, STDP): unsupervised learning</li>
</ul>
<h3 id=neural-optimal-control-hierachy-noch>Neural optimal control hierachy (NOCH)</h3>
<p>Computational model by students of Eliasmith, including:</p>
<ul>
<li>Cortex (premotor)</li>
<li>cerebellum</li>
<li>basal ganglia</li>
<li>motor cortex</li>
<li>brain stem and spinal cord</li>
</ul>
<h3 id=performing-movement-in-robot-arms>Performing movement in robot arms</h3>
<ul>
<li>Joint angle space [θ1, θ2, &mldr;]: degree of freedom</li>
<li>Operational space (end point vector)</li>
</ul>
<p>High level -> mid level -> low level control signals</p>
<p>Similar to the latter half of autoencoder.</p>
<h3 id=functional-level-model>Functional level model</h3>
<p>Loop of</p>
<ul>
<li>Cortex: memory / transformations, crude selection</li>
<li>Basal ganglia: utility -> action (cosine similarity)</li>
<li>Thalamus: monitoring</li>
</ul>
<h3 id=rules-for-manipulation>Rules for manipulation</h3>
<ul>
<li>Symbols, fuzzy logic, but not compatible to neural networks</li>
<li>Basal ganglia: manipulation
$$
\vec{s} = M_b \cdot \vec{w}
$$</li>
<li>Rehearsal of alphabet <code>sequence.py</code></li>
</ul>
<h3 id=attention>Attention</h3>
<p>Timing of neuron&rsquo;s response: ~15ms delay to make decision.</p>
<p>The less utility difference, the longer the latency.</p>
<ul>
<li>Parametric study on computational models</li>
</ul>
<h3 id=tower-of-hanoi-task>Tower of Hanoi task</h3>
<ul>
<li>Perceptural strategy from symbolic calculation is not biologically plausible in Eliasmith paper (not learning the rule).</li>
<li>150k neurons</li>
</ul>
<h3 id=act-r-architecture>ACT-R architecture</h3>
<p>Symbol -> neural networks</p>
<p>Comparative to fMRI BOLD signal.</p>
<h2 id=learning-and-memory>Learning and memory</h2>
<p>Ref: Neuroeconomics, declision making and the brain.</p>
<p>Learning: stimulus altered behavior. Not hardwired.</p>
<p>Memory: storage of learned information.</p>
<h3 id=learning-in-biology>Learning in biology</h3>
<ul>
<li>Neural level: synapse strength, neural gene expression</li>
<li>Brain regions: coordination</li>
</ul>
<h3 id=machine-learning>Machine learning</h3>
<ul>
<li>Weight changes in synaptic connections</li>
<li>Neural activity states: dynamic stability (attractor)</li>
</ul>
<h3 id=biological-memories-in-detail>Biological memories in detail</h3>
<ul>
<li>Declarative (explicit) memory: medial temporal lobe and neocortex
<ul>
<li>Events (episodic): 5W1H, past experience</li>
<li>Facts (semantic): grammar, common sense (context-free)</li>
</ul>
</li>
<li>Non-declarative memory
<ul>
<li>Procedual: basal ganglia</li>
<li>Perceptual priming: short path for recall for previous stimuli</li>
<li>Conditioning: cerebellum</li>
<li>Non-associative: reflex</li>
</ul>
</li>
<li>Sensory memory: buffer
<ul>
<li>9-10 sec for schoic (hearing)</li>
<li>0.5 sec for iconic (vision)</li>
</ul>
</li>
</ul>
<h3 id=conditioning>Conditioning</h3>
<ul>
<li>Pavlov&rsquo;s dog: classical conditioning</li>
<li>Skinner: operant conditioning</li>
<li>Acquisition, extinction, spontaneous recovery (long-term memory)</li>
</ul>
<h4 id=terms>Terms</h4>
<ul>
<li>Memory: recall / recognize past experience</li>
<li>Conditioning: associate event and response</li>
<li>Learning: change behavior to stimuli</li>
<li>Plasticity: change neural connections
<ul>
<li>Functional: chemical connection change</li>
<li>Structural: physical connection change</li>
</ul>
</li>
</ul>
<h4 id=hippocampus>Hippocampus</h4>
<p>Dentate gyrus -> CA3 -> CA1</p>
<ul>
<li>Long-term potentiation (LTP) upon high freq stimulation: enhances EPSP</li>
<li>Long-term depression (LTD) upon los freq stimulation: inhibits EPSP</li>
<li>Neural growth even at 40 y/o</li>
</ul>
<h4 id=inside-ltp--ltd>Inside LTP / LTD</h4>
<p>Neurotransmitters</p>
<ul>
<li>Glutamate (AMPAR, NMDAR) : excitary</li>
<li>GABA: inhibitory</li>
</ul>
<p>Second messengers (mid-term effcts)</p>
<h3 id=learning-rules>Learning rules</h3>
<h4 id=hebbian>Hebbian</h4>
<ul>
<li>
<p>Freud -> Hebb (1949): fire together, wire together</p>
<p>$
\Delta w = \epsilon\gamma_i\gamma_j
$</p>
<p>$\epsilon$: learning rate</p>
<p>$\gamma_i$: postsynaptic firing rate</p>
<p>$\gamma_j$: presynaptic firing rate</p>
</li>
</ul>
<h4 id=stdp>STDP</h4>
<ul>
<li>Spike-time-dependent plasticity from experimental data</li>
<li>Pre synaptic spike then post one: LTP</li>
<li>Post synaptic spike then pre one: LTD</li>
</ul>
<h4 id=hpes-rule>hPES rule</h4>
<p>Limitations on weight change</p>
<p>$$
\Delta w_{ij} = \alpha_ja_{j}(k_1e_jE + k_2a_i(a_j - \theta))
$$</p>
<h3 id=reinforcement-learning>Reinforcement learning</h3>
<p>E.g. operant conditioning (Skinner)</p>
<h4 id=value>Value</h4>
<ul>
<li>Expected value $E[ x ]$</li>
<li>Expected utility $U(E[ x ]) \approx log(E[ x ])$</li>
<li>Basic axiomatic form (Pareto)</li>
<li>Weak axioms of revealed perference (WARP)</li>
<li>Generated axioms of revealed perference (GARP)</li>
</ul>
<h4 id=value-function-vs-and-prediction-error>Value function V(s) and prediction error</h4>
<p>$V_{k+1}(s_k) = (1-\alpha)V_k(s_k) + \alpha\delta_k$</p>
<p>Error: $\delta_k = r_k - V_k(s_k)$</p>
<p>For multiple stimuli: Rescorla-Wagner model</p>
<p>$V_k^{net} = \Sigma V_{k}(stim)$</p>
<h4 id=biological-rl>Biological RL</h4>
<p>Dopamine reward pathway for movement and motivation.</p>
<p>Increased dopamine secretion for a sudden reward. The same as Error: $\delta_k = r_k - V_k(s_k)$</p>
<h4 id=decision-making>Decision making</h4>
<ul>
<li>
<p>Problem: no immediate ffeedback (reward) => need to think about the future and maximize aggregate reward</p>
</li>
<li>
<p>Bellman equation: reduction of recursive reward with temporal difference ($V_k(S_{t+1})- V_k(S_t)$)</p>
<p>$V(S_t) = r(S_t) + E[V(S_{t+1})|S_t]$</p>
<p>$\delta_t = r_t + V_k(S_{t+1})- V_k(S_t)$</p>
</li>
<li>
<p>Markov decision process</p>
</li>
<li>
<p>Q learning</p>
<ul>
<li>Q function $Q(s, \pi)$</li>
<li>Policy $\pi(s)$: mapping state to actions</li>
</ul>
<p>$Q_{t+1}(S_t, a_t) = Q_{t}(S_t, a_t) + \alpha\delta_t$</p>
<p>$\delta_t = r_t \gamma_{max}Q_{t+1}(S_t, a_t) - Q_{t}(S_t, a_t)$</p>
</li>
</ul>
<h2 id=spaun-model>SPAUN model</h2>
<p>SPAUN = Semantic pointer architecture unified network, all things put together</p>
<ul>
<li>Single perceptual system (eye)</li>
<li>Single motor system (arm)</li>
<li>Background knowledge (SPA)</li>
<li>Abilities
<ul>
<li>Smiliar to human in working mem limitations (3-7)</li>
<li>Behavior flexibility</li>
<li>Adaptation to reward</li>
<li>Confusion to invalid input</li>
</ul>
</li>
</ul></div><div class=post-footer id=post-footer>
<div class=post-info>
<div class=post-info-line>
<div class=post-info-mod>
<span>Updated on 2021-06-18&nbsp;<a class=git-hash href=https://github.com/sosiristseng/sosiristseng.github.io/commit/02d0bed84c96cb931b0f5ec31d121bd27b56bd39 target=_blank title="commit by Wen-Wei Tseng(sosiristseng@gmail.com) 02d0bed84c96cb931b0f5ec31d121bd27b56bd39: course notes" rel="noopener noreferrer">
<i class="fas fa-hashtag fa-fw"></i>02d0bed8</a></span>
</div>
<div class=post-info-license></div>
</div>
<div class=post-info-line>
<div class=post-info-md><span>
<a class=link-to-markdown href=/computational-cognitive-neuroscience/index.md target=_blank rel="noopener noreferrer">Read Markdown</a>
</span></div>
<div class=post-info-share>
<span><a href=# onclick=return!1 title="Share on Twitter" data-sharer=twitter data-url=https://sosiristseng.github.io/computational-cognitive-neuroscience/ data-title="Computational Cognitive Neuroscience" data-via=sosiristseng><i class="fab fa-twitter fa-fw"></i></a><a href=# onclick=return!1 title="Share on Facebook" data-sharer=facebook data-url=https://sosiristseng.github.io/computational-cognitive-neuroscience/><i class="fab fa-facebook-square fa-fw"></i></a><a href=# onclick=return!1 title="Share on Linkedin" data-sharer=linkedin data-url=https://sosiristseng.github.io/computational-cognitive-neuroscience/><i class="fab fa-linkedin fa-fw"></i></a><a href=# onclick=return!1 title="Share on Hacker News" data-sharer=hackernews data-url=https://sosiristseng.github.io/computational-cognitive-neuroscience/ data-title="Computational Cognitive Neuroscience"><i class="fab fa-hacker-news fa-fw"></i></a><a href=# onclick=return!1 title="Share on Reddit" data-sharer=reddit data-url=https://sosiristseng.github.io/computational-cognitive-neuroscience/><i class="fab fa-reddit fa-fw"></i></a><a href=# onclick=return!1 title="Share on Line" data-sharer=line data-url=https://sosiristseng.github.io/computational-cognitive-neuroscience/ data-title="Computational Cognitive Neuroscience"><i data-svg-src=/lib/simple-icons/icons/line.min.svg></i></a></span>
</div>
</div>
</div>
<div class=post-info-more>
<section class=post-tags></section>
<section>
<span><a href=# onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span>
</section>
</div>
<div class=post-nav><a href=/academic-writing-week-4/ class=prev rel=prev title="Academic Writing Week 4"><i class="fas fa-angle-left fa-fw"></i>Academic Writing Week 4</a>
<a href=/applied-electricity/ class=next rel=next title="Applied electricity">Applied electricity<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
</article></div>
</main><footer class=footer>
<div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.88.1">Hugo</a> | Theme - <a href=https://github.com/HEIGE-PCloud/DoIt target=_blank rel="noopener noreffer" title="DoIt 0.2.10"><i class="far fa-edit fa-fw"></i> DoIt</a>
</div><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2020 - 2021</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://sosiristseng.github.io/ target=_blank rel="noopener noreferrer">Wen-Wei Tseng</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div>
</div><script>'serviceWorker'in navigator&&(navigator.serviceWorker.register('/sw.min.js',{scope:'/'}).then(function(a){}),navigator.serviceWorker.ready.then(function(a){}))</script></footer></div>
<div id=fixed-buttons><a href=#back-to-top id=back-to-top-button class=fixed-button title="Back to Top">
<i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments">
<i class="fas fa-comment fa-fw"></i>
</a>
</div><div class=assets><script type=text/javascript src=/lib/autocomplete/autocomplete.min.js></script><script type=text/javascript src=/lib/fuse/fuse.min.js></script><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/topbar/topbar.min.js></script><script type=text/javascript src=/lib/pjax/pjax.min.js></script><script type=text/javascript src=/js/theme.min.js></script></div>
<div class=pjax-assets><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/sharer/sharer.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js></script><script type=text/javascript src=/lib/katex/auto-render.min.js></script><script type=text/javascript src=/lib/katex/copy-tex.min.js></script><script type=text/javascript src=/lib/katex/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:15},comment:{},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{distance:100,findAllMatches:!1,fuseIndexURL:"/index.json",highlightTag:"em",ignoreFieldNorm:!1,ignoreLocation:!1,isCaseSensitive:!1,location:0,maxResultLength:10,minMatchCharLength:2,noResultsFound:"No results found",snippetLength:30,threshold:.3,type:"fuse",useExtendedSearch:!1},sharerjs:!0}</script><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/katex/copy-tex.min.css></div>
</body>
</html>